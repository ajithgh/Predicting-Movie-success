---
title: "New"
author: "Ajith Hegde"
date: "12/13/2020"
output:
  word_document: default
  html_document: default
---

# Importing File

## Reading CSV
```{r}
getwd()

movies<- read.csv("rv_bud.csv", header = TRUE, stringsAsFactors = TRUE)
```

## Raw Data summarisation.
```{r}
head(movies)
dim(movies)
str(movies)
```


## Installing required libraries
```{r}
library(plyr)
library(readr)
library(ggplot2)
library(GGally)
library(dplyr)
library(mlbench)
library(dplyr)
library(tidyverse)
library(dataQualityR)
library(ggplot2)
library(ROSE)
library(rpart)
library(gbm)
library(mlbench)
library(caret)
library(jsonlite)
#install.packages("fastDummies")
library(heatmaply)
library(naniar)
library(mice)
library(randomForest)
library(caret)
library(fastDummies)

```

## Checking Raw Data quality
 + Script loads a file and produces a data quality report for
 + numerical variables: "dq_num.csv"
 + categorical variables: "dq_cat.cs"
```{r}
checkDataQuality(data = movies, 
                 out.file.num ="dq_num.csv", 
                 out.file.cat= "dq_cat.csv")
dq_num<-read.csv("dq_num.csv")
dq_cat<-read.csv("dq_cat.csv")
dq_num
dq_cat
```

#Data Imputation

## Converting 0s to NAs
```{r}
movies$revenue[movies$revenue == 0] = NA
movies$budget[movies$budget == 0] = NA
movies$revenue[((movies$budget > 100000) && (movies$revenue < 1000))] = NA
movies$budget[movies$revenue > 100000 & movies$budget < 1000] = NA
movies$runtime[movies$runtime == 0] = NA
movies$popularity[movies$popularity == 0] = NA
movies$vote_average[movies$vote_average == 0] = NA
movies$vote_count[movies$vote_count == 0] = NA
movies$review_count[movies$review_count == 0] = NA
movies$cast_size[movies$cast_size == 0] = NA
movies$crew_size[movies$crew_size == 0] = NA
```

## Extracting Release Date
```{r}
movies$release_d = as.Date(movies$release_date, "%Y-%m-%d")
movies$release_year = format(as.POSIXct(strptime(movies$release_d,"%Y-%m-%d", tz="")) ,format = "%Y")
movies$release_month = format(as.POSIXct(strptime(movies$release_d,"%Y-%m-%d", tz="")) ,format = "%m")
#view(moviessamp)
movies$release_month = as.integer(movies$release_month)
```

## Holiay month release
```{r}
movies$holidaymonth = ifelse((movies$release_month == 12 |movies$release_month == 06 | movies$release_month == 07) , 1, 0)
table(movies$holidaymonth)
```
## Converting text features to Binomial and then subsetting it.
+ tagline
+ belongs to collection
```{r}
movies$taglinepresent = ifelse(movies$tagline == "", 0, 1)
table(movies$taglinepresent)
movies$collectionyes = ifelse(movies$belongs_to_collection == "", 0, 1)
table(movies$collectionyes)
movies2 = subset(movies, select = -c( belongs_to_collection, tagline))
```


## Sample Method using MICE Package for numerical
```{r}

samp = subset(movies2, select = c(revenue, budget, runtime, popularity, vote_average, vote_count, review_count, cast_size, crew_size, release_month))

# Using MICE, missing values are imputed through a linear regression prediction based on the existing values

iv_samp = mice(data = samp, m = 5, method = "sample", maxit = 5)
num_c_samp = complete(iv_samp, 1)

moviessamp = subset(movies2, select = -c(revenue, budget, runtime, popularity, vote_average, vote_count, review_count, cast_size, crew_size, release_month))
moviessamp = cbind(moviessamp, num_c_samp)
sum(is.na(moviessamp))
#View(moviessamp)
```


## Create New Features
+ profit_factor
+ cast_crew_ratio
+ Target
```{r}
moviessamp$profit_factor = ((moviessamp$revenue - moviessamp$budget) / moviessamp$budget)*100
moviessamp$cast_crew_ratio = moviessamp$cast_size / movies$crew_size
#movies$targetprofit = ifelse(movies$profit_factor < 1, 0, ifelse(movies$profit_factor >= 1 & movies$profit_factor < 2, 1, 2))
moviessamp$Target = ifelse(moviessamp$profit_factor < 1, 0, 1)
table(moviessamp$Target)
```

## Sample Method using MICE Package for cast-crew-ratio and holdidaymonths
```{r}

samp2 = subset(moviessamp, select = c(cast_crew_ratio, holidaymonth))

# Using MICE, missing values are imputed through a linear regression prediction based on the existing values

iv_samp2 = mice(data = samp2, m = 5, method = "sample", maxit = 5)
num_c_samp2 = complete(iv_samp2, 1)

moviessamp2 = subset(moviessamp, select = -c(cast_crew_ratio, holidaymonth))
moviessamp2 = cbind(moviessamp2, num_c_samp)
sum(is.na(moviessamp2))
#View(moviessamp)
```

## Removing Unwanted columns and converting to factors
```{r}
#colnames(moviessamp)
df = subset(moviessamp2, select = -c(X.1, X, original_title, backdrop_path, homepage, imdb_id, id, poster_path, release_date, status, title, video, keywords, director, production_companies, release_year, release_d))

#df3 = subset(moviessamp, select = c(revenue, budget, targetprofit))


#str(df)

df$release_month = as.factor(df$release_month) 
str(df$release_month)
```

# Dummify Factors

## Dummify Factor Columns
```{r}

dummy = dummy_cols(
  df,
  select_columns = c("original_language", "adult", "genre", "spoken_languages", "production_countries", "release_month"),
  split = ",",
  remove_selected_columns = TRUE
)

dim(dummy)
sum(is.na(dummy))

```

----

#### Improve the naming of attributes
```{r}
library(stringr)

names(dummy)<-str_replace_all(names(dummy)," ","_")
names(dummy)<-str_replace_all(names(dummy),"_/_","_")
names(dummy)<-str_replace_all(names(dummy),",_","_")
names(dummy)<-str_replace_all(names(dummy),":_","_")
names(dummy)<-str_replace_all(names(dummy),"/","_")
names(dummy)<-str_replace_all(names(dummy),"-","_")

names(dummy)
```



# Initalising Y and X
```{r}
# library(randomForest)
# library(caret)

outcomeName <- 'Target'
predictorNames <- names(dummy)[names(dummy) != outcomeName]

finaldata = subset(dummy, select = -c(profit_factor, revenue, revenue.1 ))
```


## RF Model for feature selection 
```{r}
set.seed(103)

split = 0.8

# After cleaning the data, predictions can now be made on the feature importance
# First thing we do is create testing and training sets

index = createDataPartition(finaldata$Target, p= split, list=FALSE)

train_data = finaldata[index, ]
test_data  = finaldata[-index, ]


train_data$Target = as.factor(train_data$Target)


# In order to trim down the number of features to be used, a Random Forest model is used with bagging to find the most important ones using the randomForest model with 300 trees and 9 features per bag 9 features was chosen as it is close to the square root of the feature size.


rfModel = randomForest(Target ~ ., 
                       data = train_data,
                       ntree = 300,
                       mtry = 9,
                       replace = T)



varImpPlot(rfModel)

```




## Top 50 Important features from RF
```{r}
var_importance = cbind(names(subset(finaldata, select = -c(Target))), varImp(rfModel))
var_importance = arrange(var_importance, desc(Overall))

var_importance$Features = var_importance$`names(subset(finaldata, select = -c(Target)))`

col_names = as.data.frame(var_importance$Features[1:50])

names(df) %in% col_names$`var_importance$Features[1:50]`

important_data = subset(finaldata, select = names(finaldata) %in% col_names$`var_importance$Features[1:50]`)
important_data$Target = finaldata$Target
```

Once we have a dataset of only 25 of the most important features we can balance the dataset to an 50:50 ratio using the ROSE package

## ROSE Sampling
```{r}
important_data_balanced = ovun.sample(Target~., data = important_data, method = "both",  p=0.6)$data
prop.table(table(important_data_balanced$Target))

train_data = important_data_balanced[index, ]
test_data  = important_data_balanced[-index, ]
train_data$Target = as.factor(train_data$Target)
```


Once we have the most important features, and a balanced dataset,  the first model to run is Logistic Regression using lgm

# Logistic Regression

## Predicting using GLM Model 
```{r}
logModel = glm(Target ~ ., data = train_data, family = "binomial", maxit = 100)

predictions_log = as.data.frame(as.factor(ifelse(predict(logModel, 
                                                         test_data, 
                                                         type = "response")>0.5, 1, 0)))

```

Using caret and ROSE, a confusion matrix and ROC curve are created respectively

## Confusion Matrix for above glm model
```{r}
test_data$Target = as.factor(test_data$Target)

confusionMatrix(predictions_log$`as.factor(ifelse(predict(logModel, test_data, type = "response") > 0.5, 1, 0))`, 
                test_data$Target)

roc.curve(test_data$Target, 
          predictions_log$`as.factor(ifelse(predict(logModel, test_data, type = "response") > 0.5, 1, 0))`,
          plotit = T,
          main = "Logistic Regression Model")
```

## Predicting using RF Model
The second model to run is a repeat of the one used when selecting features
```{r}
rfModel2 = randomForest(Target ~ ., 
                       data = train_data,
                       ntree = 300, 
                       mtry = 9, 
                       replace = T) 
predictions_rf2 = as.data.frame(predict(rfModel2, 
                                       test_data, 
                                       type = "response"))
```


```{r}
confusionMatrix(predictions_rf2$`predict(rfModel2, test_data, type = "response")`,
                test_data$Target)

roc.curve(test_data$Target, 
          predictions_rf2$`predict(rfModel2, test_data, type = "response")`,
          plotit = T,
          main = "ROC Curve for Random Forest w/ Bagging Model")
```

# Gabriela's Feature Engineering

## Feature 1
```{r}

```

## Feature 2
```{r}

```

